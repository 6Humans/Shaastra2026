"""
Production-Grade Data Quality Assessment Module

STRICT RULES:
- NO simulated weights
- NO heuristic guesses
- NO placeholder logic
- NO hard-coded scores
- ALL insights must be generated via real LLM API calls
- ALL metrics must be computed dynamically from data

Implements 6 key data quality dimensions:
1. Completeness - % of non-null values
2. Uniqueness - % of non-duplicate records
3. Validity - % passing format/business rules checks
4. Consistency - % of values following expected patterns
5. Accuracy - Outlier detection using Isolation Forest
6. Timeliness - Freshness of data based on date columns

All scores normalized to 0-100 scale.
"""

import os
import re
import asyncio
import warnings
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional

import pandas as pd
import numpy as np
import httpx
from dotenv import load_dotenv
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor

# Load environment variables from .env file
load_dotenv()


class DataQualityAssessment:
    """Production-grade data quality scoring with real LLM insights."""
    
    def __init__(self, df: pd.DataFrame):
        """
        Initialize with a DataFrame.
        
        Args:
            df: Pandas DataFrame to assess
        """
        self.df = df.copy()
        self.total_rows = len(df)
        self.total_cols = len(df.columns)
        self.total_cells = self.total_rows * self.total_cols
        
        # Validation error tracking
        self.validation_errors: List[Dict[str, Any]] = []
        self.validation_warnings: List[Dict[str, Any]] = []
        self.critical_failures: List[Dict[str, Any]] = []
        
        # LLM configuration (NO HARDCODING - from environment)
        self.api_key = os.getenv('OPENROUTER_API_KEY')
        self.api_base = os.getenv('OPENROUTER_API_BASE', 'https://openrouter.ai/api/v1/chat/completions')
        self.model = os.getenv('OPENROUTER_MODEL', 'qwen/qwen3-235b-a22b-2507')
        
        # Warn if LLM not available, but allow metric computation
        if not self.api_key:
            print("‚ö†Ô∏è  WARNING: OPENROUTER_API_KEY not set - LLM insights will be unavailable")
            print("   All metric computations will still work (no simulation)")
        else:
            print(f"‚úÖ LLM INSIGHTS ENABLED: API key loaded (length: {len(self.api_key)})")

        
    def calculate_completeness(self) -> Dict[str, Any]:
        """
        Completeness: % of non-null values
        Formula: 100 √ó (total_cells - missing_cells) / total_cells
        """
        print("\nüî¢ [COMPLETENESS] Computing...")
        missing_cells = self.df.isnull().sum().sum()
        filled_cells = self.total_cells - missing_cells
        score = (filled_cells / self.total_cells * 100) if self.total_cells > 0 else 0
        print(f"   üìä Formula: 100 √ó ({filled_cells} / {self.total_cells}) = {score:.2f}")
        print(f"   ‚öñÔ∏è  Weight: Direct calculation (no weight applied)")
        
        # Per-column completeness
        column_scores = {}
        for col in self.df.columns:
            col_missing = self.df[col].isnull().sum()
            col_score = ((self.total_rows - col_missing) / self.total_rows * 100) if self.total_rows > 0 else 0
            column_scores[col] = round(col_score, 2)
        
        return {
            'score': round(score, 2),
            'total_cells': self.total_cells,
            'filled_cells': int(filled_cells),
            'missing_cells': int(missing_cells),
            'column_scores': column_scores,
            'description': 'Percentage of non-null values across all cells'
        }
    
    def calculate_uniqueness(self) -> Dict[str, Any]:
        """
        Uniqueness: % of non-duplicate records
        Formula: 100 √ó (1 - duplicates / total_rows)
        """
        print("\nüî¢ [UNIQUENESS] Computing...")
        duplicate_rows = self.df.duplicated().sum()
        unique_rows = self.total_rows - duplicate_rows
        score = (unique_rows / self.total_rows * 100) if self.total_rows > 0 else 0
        print(f"   üìä Formula: 100 √ó (1 - {duplicate_rows}/{self.total_rows}) = {score:.2f}")
        print(f"   ‚öñÔ∏è  Weight: Direct calculation (no weight applied)")
        
        # Per-column uniqueness
        column_uniqueness = {}
        for col in self.df.columns:
            unique_count = self.df[col].nunique()
            col_score = (unique_count / self.total_rows * 100) if self.total_rows > 0 else 0
            column_uniqueness[col] = {
                'unique_values': int(unique_count),
                'uniqueness_pct': round(col_score, 2)
            }
        
        return {
            'score': round(score, 2),
            'total_rows': self.total_rows,
            'unique_rows': int(unique_rows),
            'duplicate_rows': int(duplicate_rows),
            'duplicate_percentage': round((duplicate_rows / self.total_rows * 100) if self.total_rows > 0 else 0, 2),
            'column_uniqueness': column_uniqueness,
            'description': 'Percentage of non-duplicate records'
        }
    
    def calculate_validity(self) -> Dict[str, Any]:
        """
        Validity: % of values passing format/business rule checks
        Checks: email format, phone format, card numbers (Luhn), dates, numeric ranges
        """
        validation_results = {}
        total_validations = 0
        passed_validations = 0
        
        for col in self.df.columns:
            col_data = self.df[col].dropna()
            if len(col_data) == 0:
                continue
            
            col_validations = {
                'total': len(col_data),
                'passed': len(col_data),
                'checks_applied': []
            }
            
            # Email validation
            if 'email' in col.lower():
                email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
                valid = col_data.astype(str).str.match(email_pattern).sum()
                col_validations['passed'] = min(col_validations['passed'], int(valid))
                col_validations['checks_applied'].append('email_format')
                total_validations += len(col_data)
                passed_validations += valid
            
            # Phone validation
            elif 'phone' in col.lower() or 'mobile' in col.lower():
                phone_pattern = r'^\+?[1-9]\d{1,14}$|^\d{10}$'
                valid = col_data.astype(str).str.match(phone_pattern).sum()
                col_validations['passed'] = min(col_validations['passed'], int(valid))
                col_validations['checks_applied'].append('phone_format')
                total_validations += len(col_data)
                passed_validations += valid
            
            # Card number validation (Luhn algorithm)
            elif 'card' in col.lower() and 'number' in col.lower():
                def luhn_check(card_num):
                    try:
                        card_str = str(card_num).replace(' ', '').replace('-', '')
                        if not card_str.isdigit() or len(card_str) < 13:
                            return False
                        
                        digits = [int(d) for d in card_str]
                        checksum = 0
                        for i in range(len(digits) - 2, -1, -2):
                            doubled = digits[i] * 2
                            checksum += doubled if doubled < 10 else doubled - 9
                        for i in range(len(digits) - 1, -1, -2):
                            checksum += digits[i]
                        return checksum % 10 == 0
                    except:
                        return False
                
                valid = col_data.apply(luhn_check).sum()
                col_validations['passed'] = min(col_validations['passed'], int(valid))
                col_validations['checks_applied'].append('luhn_algorithm')
                total_validations += len(col_data)
                passed_validations += valid
            
            # Date format validation
            elif 'date' in col.lower() or 'time' in col.lower():
                try:
                    with warnings.catch_warnings():
                        warnings.simplefilter("ignore")
                        converted = pd.to_datetime(col_data, errors='coerce', format='mixed')
                    
                    valid = converted.notna().sum()
                    invalid = len(col_data) - valid
                    
                    # Log type safety issues
                    if invalid > 0:
                        invalid_samples = col_data[converted.isna()].head(5).tolist()
                        self.validation_errors.append({
                            'dimension': 'validity',
                            'column': col,
                            'error_type': 'invalid_date_format',
                            'severity': 'high',
                            'count': int(invalid),
                            'percentage': round(invalid / len(col_data) * 100, 2),
                            'samples': invalid_samples,
                            'message': f'{invalid} values could not be parsed as dates'
                        })
                    
                    col_validations['passed'] = int(valid)
                    col_validations['checks_applied'].append('date_format')
                    total_validations += len(col_data)
                    passed_validations += valid
                except Exception as e:
                    self.critical_failures.append({
                        'dimension': 'validity',
                        'column': col,
                        'error': str(e),
                        'message': f'Critical failure in date validation for {col}'
                    })
            
            # Numeric range validation (no negative amounts)
            elif 'amount' in col.lower() or 'fee' in col.lower() or 'price' in col.lower():
                if pd.api.types.is_numeric_dtype(col_data):
                    valid = (col_data >= 0).sum()
                    col_validations['passed'] = min(col_validations['passed'], int(valid))
                    col_validations['checks_applied'].append('positive_numeric')
                    total_validations += len(col_data)
                    passed_validations += valid
            
            # Status/category validation (non-empty strings)
            elif 'status' in col.lower() or 'category' in col.lower() or 'type' in col.lower():
                valid = col_data.astype(str).str.strip().str.len().gt(0).sum()
                col_validations['passed'] = min(col_validations['passed'], int(valid))
                col_validations['checks_applied'].append('non_empty_string')
                total_validations += len(col_data)
                passed_validations += valid
            
            if col_validations['checks_applied']:
                validation_results[col] = col_validations
        
        score = (passed_validations / total_validations * 100) if total_validations > 0 else 100
        
        print("\nüî¢ [VALIDITY] Computing...")
        print(f"   üìä Formula: 100 √ó ({passed_validations}/{total_validations}) = {score:.2f}")
        print(f"   ‚öñÔ∏è  Weight: Direct calculation (no weight applied)")
        print(f"   ‚úÖ Checks applied: Luhn, Email, Phone, Date, Numeric ranges")
        
        return {
            'score': round(score, 2),
            'total_validations': int(total_validations),
            'passed_validations': int(passed_validations),
            'failed_validations': int(total_validations - passed_validations),
            'column_validations': validation_results,
            'description': 'Percentage of values passing format and business rule checks'
        }
    
    def calculate_consistency(self) -> Dict[str, Any]:
        """
        Consistency: % of values following expected patterns and data types
        Checks: data type consistency, pattern consistency, cross-field consistency
        """
        consistency_checks = []
        total_checks = 0
        passed_checks = 0
        
        for col in self.df.columns:
            col_data = self.df[col].dropna()
            if len(col_data) == 0:
                continue
            
            # Data type consistency (% matching most common type)
            type_counts = col_data.apply(type).value_counts()
            dominant_type = type_counts.index[0]
            type_consistent = (col_data.apply(type) == dominant_type).sum()
            total_checks += len(col_data)
            passed_checks += type_consistent
            
            consistency_checks.append({
                'column': col,
                'check': 'data_type_consistency',
                'total': len(col_data),
                'passed': int(type_consistent),
                'score': round((type_consistent / len(col_data) * 100), 2)
            })
            
            # String pattern consistency (for text columns)
            if pd.api.types.is_string_dtype(col_data) or pd.api.types.is_object_dtype(col_data):
                str_data = col_data.astype(str)
                
                # Case consistency
                upper_count = str_data.str.isupper().sum()
                lower_count = str_data.str.islower().sum()
                title_count = str_data.str.istitle().sum()
                max_case = max(upper_count, lower_count, title_count)
                
                if len(str_data) > 0:
                    total_checks += len(str_data)
                    passed_checks += max_case
                    consistency_checks.append({
                        'column': col,
                        'check': 'case_consistency',
                        'total': len(str_data),
                        'passed': int(max_case),
                        'score': round((max_case / len(str_data) * 100), 2)
                    })
        
        # Cross-field consistency checks
        date_cols = [col for col in self.df.columns if 'date' in col.lower() or 'time' in col.lower()]
        if len(date_cols) >= 2:
            # Check if dates are in logical order (e.g., created_date < updated_date)
            for i in range(len(date_cols) - 1):
                try:
                    with warnings.catch_warnings():
                        warnings.simplefilter("ignore")
                        date1 = pd.to_datetime(self.df[date_cols[i]], errors='coerce', format='mixed')
                        date2 = pd.to_datetime(self.df[date_cols[i + 1]], errors='coerce', format='mixed')
                    
                    # Track parsing failures
                    date1_invalid = date1.isna().sum()
                    date2_invalid = date2.isna().sum()
                    
                    if date1_invalid > 0 or date2_invalid > 0:
                        self.validation_warnings.append({
                            'dimension': 'consistency',
                            'columns': [date_cols[i], date_cols[i + 1]],
                            'warning_type': 'date_parsing_issues',
                            'date1_invalid': int(date1_invalid),
                            'date2_invalid': int(date2_invalid),
                            'message': f'Date parsing issues in cross-field validation'
                        })
                    
                    valid_pairs = ((date1 <= date2) | date1.isna() | date2.isna()).sum()
                    total_pairs = len(self.df)
                    
                    total_checks += total_pairs
                    passed_checks += valid_pairs
                    consistency_checks.append({
                        'column': f'{date_cols[i]} vs {date_cols[i + 1]}',
                        'check': 'date_order_consistency',
                        'total': total_pairs,
                        'passed': int(valid_pairs),
                        'score': round((valid_pairs / total_pairs * 100), 2)
                    })
                except:
                    pass
        
        score = (passed_checks / total_checks * 100) if total_checks > 0 else 100
        
        print("\nüî¢ [CONSISTENCY] Computing...")
        print(f"   üìä Formula: 100 √ó ({passed_checks}/{total_checks}) = {score:.2f}")
        print(f"   ‚öñÔ∏è  Weight: Direct calculation (no weight applied)")
        print(f"   ‚úÖ Checks: Type consistency, Pattern consistency, Cross-field validation")
        
        return {
            'score': round(score, 2),
            'total_checks': int(total_checks),
            'passed_checks': int(passed_checks),
            'failed_checks': int(total_checks - passed_checks),
            'consistency_details': consistency_checks,
            'description': 'Percentage of values following expected patterns and consistency rules'
        }
    
    def calculate_accuracy(self) -> Dict[str, Any]:
        """
        Accuracy: Outlier detection using Isolation Forest (ML-based)
        Score: 100 √ó (1 - outliers / total_rows)
        """
        numeric_cols = self.df.select_dtypes(include=[np.number]).columns.tolist()
        
        if not numeric_cols:
            return {
                'score': 100.0,
                'outliers_detected': 0,
                'outlier_percentage': 0.0,
                'column_outliers': {},
                'description': 'No numeric columns for outlier detection',
                'method': 'Isolation Forest (ML)'
            }
        
        column_outliers = {}
        total_outliers = 0
        
        for col in numeric_cols:
            col_data = self.df[col].dropna()
            
            if len(col_data) < 10:  # Need at least 10 samples for Isolation Forest
                continue
            
            try:
                # Prepare data for Isolation Forest
                X = col_data.values.reshape(-1, 1)
                
                # Isolation Forest (contamination = expected % of outliers, default 0.1 = 10%)
                iso_forest = IsolationForest(
                    contamination=0.1,
                    random_state=42,
                    n_estimators=100
                )
                predictions = iso_forest.fit_predict(X)
                
                # -1 indicates outlier, 1 indicates inlier
                outliers = (predictions == -1).sum()
                outlier_indices = np.where(predictions == -1)[0]
                
                column_outliers[col] = {
                    'total_values': len(col_data),
                    'outliers': int(outliers),
                    'outlier_percentage': round((outliers / len(col_data) * 100), 2),
                    'outlier_sample': col_data.iloc[outlier_indices[:5]].tolist() if len(outlier_indices) > 0 else []
                }
                
                total_outliers += outliers
            except Exception as e:
                column_outliers[col] = {
                    'error': str(e),
                    'outliers': 0
                }
        
        total_numeric_values = sum(self.df[col].notna().sum() for col in numeric_cols)
        outlier_percentage = (total_outliers / total_numeric_values * 100) if total_numeric_values > 0 else 0
        score = max(0, 100 - outlier_percentage * 2)  # Penalize outliers more heavily
        
        print("\nüî¢ [ACCURACY] Computing with ML...")
        print(f"   üìä Formula: max(0, 100 - {outlier_percentage:.2f}% √ó 2) = {score:.2f}")
        print(f"   ‚öñÔ∏è  Weight: Outlier penalty multiplier = 2√ó")
        print(f"   ü§ñ ML Model: Isolation Forest (contamination=0.1, n_estimators=100)")
        print(f"   üéØ Outliers detected: {total_outliers} out of {total_numeric_values} values")
        
        return {
            'score': round(score, 2),
            'outliers_detected': int(total_outliers),
            'outlier_percentage': round(outlier_percentage, 2),
            'total_numeric_values': int(total_numeric_values),
            'column_outliers': column_outliers,
            'description': 'Outlier detection using Isolation Forest (Machine Learning)',
            'method': 'Isolation Forest (contamination=0.1)'
        }
    
    def calculate_timeliness(self) -> Dict[str, Any]:
        """
        Timeliness: Data freshness based on date columns
        Score based on how recent the data is (100 = today, decreases with age)
        """
        date_cols = [col for col in self.df.columns if 'date' in col.lower() or 'time' in col.lower()]
        
        if not date_cols:
            return {
                'score': 100.0,
                'date_columns_found': 0,
                'freshness_analysis': {},
                'description': 'No date columns found for timeliness assessment',
                'recommendation': 'Add timestamp columns to track data freshness'
            }
        
        freshness_scores = []
        freshness_analysis = {}
        current_date = pd.Timestamp.now()
        
        for col in date_cols:
            try:
                # Convert to datetime with type safety
                with warnings.catch_warnings():
                    warnings.simplefilter("ignore")
                    date_series_raw = pd.to_datetime(self.df[col], errors='coerce', format='mixed')
                
                # Track parsing failures
                invalid_count = date_series_raw.isna().sum()
                total_count = len(self.df[col])
                
                if invalid_count > 0:
                    invalid_samples = self.df[col][date_series_raw.isna()].head(5).tolist()
                    self.validation_errors.append({
                        'dimension': 'timeliness',
                        'column': col,
                        'error_type': 'unparseable_datetime',
                        'severity': 'medium',
                        'count': int(invalid_count),
                        'percentage': round(invalid_count / total_count * 100, 2),
                        'samples': invalid_samples,
                        'message': f'{invalid_count}/{total_count} datetime values could not be parsed'
                    })
                
                date_series = date_series_raw.dropna()
                
                if len(date_series) == 0:
                    self.critical_failures.append({
                        'dimension': 'timeliness',
                        'column': col,
                        'error': 'all_dates_invalid',
                        'message': f'All datetime values in {col} are invalid'
                    })
                    continue
                
                # Calculate freshness metrics
                most_recent = date_series.max()
                oldest = date_series.min()
                average_date = pd.to_datetime(date_series.mean())
                
                # Days since most recent record
                days_since_recent = (current_date - most_recent).days
                days_since_oldest = (current_date - oldest).days
                days_since_average = (current_date - average_date).days
                
                # Calculate freshness score (100 for today, decreases over time)
                # 100 points for data within last 7 days
                # 80 points for data within last 30 days
                # 60 points for data within last 90 days
                # 40 points for data within last 180 days
                # 20 points for data within last 365 days
                # 0 points for data older than 365 days
                
                if days_since_recent <= 7:
                    col_score = 100
                elif days_since_recent <= 30:
                    col_score = 80
                elif days_since_recent <= 90:
                    col_score = 60
                elif days_since_recent <= 180:
                    col_score = 40
                elif days_since_recent <= 365:
                    col_score = 20
                else:
                    col_score = 0
                
                freshness_scores.append(col_score)
                
                freshness_analysis[col] = {
                    'most_recent_date': most_recent.strftime('%Y-%m-%d'),
                    'oldest_date': oldest.strftime('%Y-%m-%d'),
                    'average_date': average_date.strftime('%Y-%m-%d'),
                    'days_since_recent': int(days_since_recent),
                    'days_since_oldest': int(days_since_oldest),
                    'data_age_range_days': int(days_since_oldest - days_since_recent),
                    'freshness_score': col_score,
                    'freshness_category': self._get_freshness_category(days_since_recent)
                }
                
            except Exception as e:
                freshness_analysis[col] = {
                    'error': str(e),
                    'freshness_score': 0
                }
        
        overall_score = np.mean(freshness_scores) if freshness_scores else 100
        
        return {
            'score': round(overall_score, 2),
            'date_columns_found': len(date_cols),
            'date_columns_analyzed': len(freshness_scores),
            'freshness_analysis': freshness_analysis,
            'description': 'Data freshness based on date/timestamp columns',
            'recommendation': self._get_timeliness_recommendation(overall_score)
        }
    
    def _get_freshness_category(self, days: int) -> str:
        """Categorize data freshness."""
        if days <= 7:
            return 'Fresh (< 7 days)'
        elif days <= 30:
            return 'Recent (< 30 days)'
        elif days <= 90:
            return 'Moderate (< 90 days)'
        elif days <= 180:
            return 'Aging (< 180 days)'
        elif days <= 365:
            return 'Old (< 1 year)'
        else:
            return 'Stale (> 1 year)'
    
    def _get_timeliness_recommendation(self, score: float) -> str:
        """Provide timeliness recommendations."""
        if score >= 80:
            return 'Data is fresh and up-to-date'
        elif score >= 60:
            return 'Consider refreshing data within 30 days'
        elif score >= 40:
            return 'Data is aging - refresh recommended within 2 weeks'
        else:
            return 'Data is stale - immediate refresh required'
    
    def calculate_integrity(self) -> Dict[str, Any]:
        """
        Integrity: Cross-field relationships and business rule validation
        Checks: Referential constraints, logical relationships, business rules
        Score: 100 √ó (passed_checks / total_checks)
        """
        print("\nüî¢ [INTEGRITY] Computing...")
        
        total_checks = 0
        passed_checks = 0
        integrity_issues = []
        integrity_details = []
        
        # 1. CROSS-FIELD NUMERIC RELATIONSHIPS
        numeric_cols = self.df.select_dtypes(include=[np.number]).columns.tolist()
        total_cols = [c for c in numeric_cols if any(kw in c.lower() for kw in ['total', 'sum', 'net', 'gross'])]
        
        for total_col in total_cols[:2]:
            try:
                non_negative = (self.df[total_col] >= 0).sum()
                total_rows = self.df[total_col].notna().sum()
                if total_rows > 0:
                    total_checks += total_rows
                    passed_checks += non_negative
                    if non_negative < total_rows:
                        integrity_issues.append({'type': 'negative_total', 'column': total_col, 'failed': int(total_rows - non_negative), 'severity': 'medium'})
                    integrity_details.append({'check': 'non_negative_total', 'column': total_col, 'total': int(total_rows), 'passed': int(non_negative), 'score': round(non_negative / total_rows * 100, 2)})
            except Exception:
                pass
        
        # 2. DATE ORDERING CONSTRAINTS
        for prefix1, prefix2 in [('created', 'updated'), ('start', 'end'), ('transaction', 'settlement')]:
            col1_matches = [c for c in self.df.columns if prefix1 in c.lower() and ('date' in c.lower() or 'time' in c.lower())]
            col2_matches = [c for c in self.df.columns if prefix2 in c.lower() and ('date' in c.lower() or 'time' in c.lower())]
            if col1_matches and col2_matches:
                col1, col2 = col1_matches[0], col2_matches[0]
                try:
                    with warnings.catch_warnings():
                        warnings.simplefilter("ignore")
                        date1 = pd.to_datetime(self.df[col1], errors='coerce', format='mixed')
                        date2 = pd.to_datetime(self.df[col2], errors='coerce', format='mixed')
                    valid_pairs = ((date1 <= date2) | date1.isna() | date2.isna())
                    valid_count = valid_pairs.sum()
                    total_checks += len(self.df)
                    passed_checks += valid_count
                    if valid_count < len(self.df):
                        integrity_issues.append({'type': 'date_order_violation', 'columns': [col1, col2], 'failed': int(len(self.df) - valid_count), 'severity': 'high'})
                    integrity_details.append({'check': 'date_ordering', 'columns': [col1, col2], 'total': len(self.df), 'passed': int(valid_count), 'score': round(valid_count / len(self.df) * 100, 2)})
                except Exception:
                    pass
        
        # 3. ID FORMAT CONSISTENCY
        for id_col in [c for c in self.df.columns if 'id' in c.lower() or 'reference' in c.lower()][:5]:
            try:
                col_data = self.df[id_col].dropna().astype(str)
                if len(col_data) > 0:
                    lengths = col_data.str.len()
                    most_common = lengths.mode().iloc[0] if len(lengths.mode()) > 0 else lengths.median()
                    consistent = (lengths == most_common).sum()
                    total_checks += len(col_data)
                    passed_checks += consistent
                    integrity_details.append({'check': 'id_format_consistency', 'column': id_col, 'total': len(col_data), 'passed': int(consistent), 'score': round(consistent / len(col_data) * 100, 2)})
            except Exception:
                pass
        
        # 4. AMOUNT REASONABILITY (IQR)
        for amount_col in [c for c in numeric_cols if any(kw in c.lower() for kw in ['amount', 'price', 'value'])][:3]:
            try:
                amount_data = self.df[amount_col].dropna()
                if len(amount_data) > 10:
                    Q1, Q3 = amount_data.quantile(0.25), amount_data.quantile(0.75)
                    IQR = Q3 - Q1
                    reasonable = ((amount_data >= Q1 - 3*IQR) & (amount_data <= Q3 + 3*IQR)).sum()
                    total_checks += len(amount_data)
                    passed_checks += reasonable
                    integrity_details.append({'check': 'amount_reasonability', 'column': amount_col, 'total': len(amount_data), 'passed': int(reasonable), 'score': round(reasonable / len(amount_data) * 100, 2)})
            except Exception:
                pass
        
        if total_checks == 0:
            score = 100.0
            print(f"   üìä No integrity checks applicable ‚Üí defaulting to 100")
        else:
            score = (passed_checks / total_checks * 100)
            print(f"   üìä Formula: 100 √ó ({passed_checks}/{total_checks}) = {score:.2f}")
        print(f"   ‚öñÔ∏è  Weight: Direct calculation")
        print(f"   ‚úÖ Checks: Cross-field, Date ordering, ID format, Amount reasonability")
        
        return {
            'score': round(score, 2),
            'total_checks': int(total_checks),
            'passed_checks': int(passed_checks),
            'failed_checks': int(total_checks - passed_checks),
            'integrity_issues': integrity_issues,
            'integrity_details': integrity_details,
            'description': 'Cross-field relationships, referential constraints, and business rule validation'
        }
    
    def calculate_dynamic_weights(self) -> Dict[str, Any]:
        """
        Calculate dynamic weights for each dimension based on EDA analysis AND operational context.
        
        Contextual Inference Strategy:
        - Transactional Context (Settlement, Logs, Transactions): Higher Timeliness, Accuracy
        - Master Context (Merchant List, KYC, Reference): Higher Validity, Completeness
        - Analytical Context (Report, Quarterly, Insights): Higher Consistency, Uniqueness
        
        Returns dict with 'weights' (normalized to 1.0) and 'reasoning_log' (audit trail)
        """
        print("\n" + "üîç"*40)
        print("‚öñÔ∏è  CALCULATING DYNAMIC WEIGHTS BASED ON EDA + CONTEXT")
        print("üîç"*40)
        
        reasoning_log = []
        column_schema = [col.lower() for col in self.df.columns]
        column_schema_str = ", ".join(self.df.columns.tolist()[:20])  # First 20 columns for display
        
        # ==========================================
        # STEP 1: CONTEXTUAL INFERENCE FROM COLUMN SCHEMA
        # ==========================================
        print("\nüìã STEP 1: Contextual Inference from Column Schema")
        print(f"   Columns: {column_schema_str}{'...' if len(self.df.columns) > 20 else ''}")
        
        # Define context detection keywords
        transactional_keywords = ['settlement', 'transaction', 'log', 'payment', 'transfer', 'debit', 'credit', 'batch', 'trace', 'auth']
        master_keywords = ['merchant', 'kyc', 'reference', 'customer', 'vendor', 'supplier', 'account', 'profile', 'master', 'registry']
        analytical_keywords = ['report', 'quarterly', 'insight', 'summary', 'aggregate', 'trend', 'analysis', 'metric', 'kpi', 'dashboard']
        
        # Count matches for each context
        transactional_matches = [kw for kw in transactional_keywords if any(kw in col for col in column_schema)]
        master_matches = [kw for kw in master_keywords if any(kw in col for col in column_schema)]
        analytical_matches = [kw for kw in analytical_keywords if any(kw in col for col in column_schema)]
        
        transactional_score = len(transactional_matches)
        master_score = len(master_matches)
        analytical_score = len(analytical_matches)
        
        # Determine primary context
        context_scores = {
            'transactional': transactional_score,
            'master': master_score,
            'analytical': analytical_score
        }
        
        inferred_context = max(context_scores, key=context_scores.get)
        max_score = max(context_scores.values())
        
        # If no clear context, default to transactional (for financial data)
        if max_score == 0:
            inferred_context = 'transactional'
            reasoning_log.append(f"No clear context keywords detected in columns. Defaulting to 'transactional' context for financial data safety.")
        else:
            reasoning_log.append(f"Context inferred: '{inferred_context.upper()}' (matched keywords: {context_scores[inferred_context]})")
            if inferred_context == 'transactional':
                reasoning_log.append(f"Transactional keywords detected: {transactional_matches}")
            elif inferred_context == 'master':
                reasoning_log.append(f"Master data keywords detected: {master_matches}")
            else:
                reasoning_log.append(f"Analytical keywords detected: {analytical_matches}")
        
        print(f"   üéØ Inferred Context: {inferred_context.upper()}")
        print(f"   üìä Context Scores: Transactional={transactional_score}, Master={master_score}, Analytical={analytical_score}")
        print(f"   üîç Matched Keywords: {transactional_matches if inferred_context == 'transactional' else master_matches if inferred_context == 'master' else analytical_matches}")
        
        # ==========================================
        # STEP 2: BASE WEIGHTS FROM EDA ANALYSIS
        # ==========================================
        print("\nüìã STEP 2: Base Weights from EDA Analysis")
        
        weights = {}
        
        # COMPLETENESS - Based on missingness
        missing_ratio = self.df.isnull().sum().sum() / self.total_cells if self.total_cells > 0 else 0
        if missing_ratio > 0.30:
            completeness_weight = 0.20
        elif missing_ratio > 0.15:
            completeness_weight = 0.16
        elif missing_ratio > 0.05:
            completeness_weight = 0.12
        else:
            completeness_weight = 0.08
        reasoning_log.append(f"Completeness base weight: {completeness_weight:.2f} (missing ratio: {missing_ratio*100:.1f}%)")
        
        # UNIQUENESS - Based on duplicates
        duplicate_ratio = self.df.duplicated().sum() / self.total_rows if self.total_rows > 0 else 0
        if duplicate_ratio > 0.20:
            uniqueness_weight = 0.18
        elif duplicate_ratio > 0.10:
            uniqueness_weight = 0.14
        elif duplicate_ratio > 0.02:
            uniqueness_weight = 0.10
        else:
            uniqueness_weight = 0.08
        reasoning_log.append(f"Uniqueness base weight: {uniqueness_weight:.2f} (duplicate ratio: {duplicate_ratio*100:.1f}%)")
        
        # VALIDITY - Based on string ratio
        string_cols = sum(1 for dt in self.df.dtypes if dt == 'object')
        string_ratio = string_cols / len(self.df.columns) if len(self.df.columns) > 0 else 0
        if string_ratio > 0.6:
            validity_weight = 0.18
        elif string_ratio > 0.4:
            validity_weight = 0.14
        else:
            validity_weight = 0.10
        reasoning_log.append(f"Validity base weight: {validity_weight:.2f} (string columns: {string_ratio*100:.1f}%)")
        
        # CONSISTENCY - Based on pattern variations
        pattern_score = 0
        for col in self.df.select_dtypes(include=['object']).columns[:10]:
            if self.df[col].notna().sum() > 0:
                if self.df[col].dropna().astype(str).str.len().nunique() > 5:
                    pattern_score += 1
        if pattern_score > 5:
            consistency_weight = 0.16
        elif pattern_score > 2:
            consistency_weight = 0.12
        else:
            consistency_weight = 0.08
        reasoning_log.append(f"Consistency base weight: {consistency_weight:.2f} (pattern variations: {pattern_score})")
        
        # ACCURACY - Based on numeric columns
        numeric_cols = self.df.select_dtypes(include=[np.number]).columns
        numeric_ratio = len(numeric_cols) / len(self.df.columns) if len(self.df.columns) > 0 else 0
        if numeric_ratio > 0.5:
            accuracy_weight = 0.18
        elif numeric_ratio > 0.3:
            accuracy_weight = 0.14
        else:
            accuracy_weight = 0.10
        reasoning_log.append(f"Accuracy base weight: {accuracy_weight:.2f} (numeric columns: {numeric_ratio*100:.1f}%)")
        
        # TIMELINESS - Based on date columns
        date_cols = [col for col in self.df.columns if 'date' in col.lower() or 'time' in col.lower()]
        if len(date_cols) >= 3:
            timeliness_weight = 0.16
        elif len(date_cols) >= 1:
            timeliness_weight = 0.12
        else:
            timeliness_weight = 0.08
        reasoning_log.append(f"Timeliness base weight: {timeliness_weight:.2f} (date columns: {len(date_cols)})")
        
        # INTEGRITY - Based on ID/reference columns
        id_cols = [c for c in self.df.columns if 'id' in c.lower() or 'reference' in c.lower()]
        amount_cols = [c for c in numeric_cols if any(kw in c.lower() for kw in ['amount', 'total', 'price'])]
        if len(id_cols) >= 3 or len(amount_cols) >= 2:
            integrity_weight = 0.14
        elif len(id_cols) >= 1 or len(amount_cols) >= 1:
            integrity_weight = 0.10
        else:
            integrity_weight = 0.08
        reasoning_log.append(f"Integrity base weight: {integrity_weight:.2f} (ID cols: {len(id_cols)}, Amount cols: {len(amount_cols)})")
        
        # ==========================================
        # STEP 3: APPLY CONTEXT-BASED MULTIPLIERS
        # ==========================================
        print("\nüìã STEP 3: Applying Context-Based Weight Adjustments")
        
        context_multiplier = 1.5  # Boost priority dimensions by 50%
        
        if inferred_context == 'transactional':
            # Priority: Timeliness, Accuracy
            timeliness_weight *= context_multiplier
            accuracy_weight *= context_multiplier
            reasoning_log.append(f"TRANSACTIONAL context: Boosted Timeliness ({timeliness_weight:.2f}) and Accuracy ({accuracy_weight:.2f}) by 50%")
            print(f"   üöÄ TRANSACTIONAL BOOST: Timeliness √ó1.5, Accuracy √ó1.5")
            
        elif inferred_context == 'master':
            # Priority: Validity, Completeness
            validity_weight *= context_multiplier
            completeness_weight *= context_multiplier
            reasoning_log.append(f"MASTER context: Boosted Validity ({validity_weight:.2f}) and Completeness ({completeness_weight:.2f}) by 50%")
            print(f"   üöÄ MASTER DATA BOOST: Validity √ó1.5, Completeness √ó1.5")
            
        elif inferred_context == 'analytical':
            # Priority: Consistency, Uniqueness
            consistency_weight *= context_multiplier
            uniqueness_weight *= context_multiplier
            reasoning_log.append(f"ANALYTICAL context: Boosted Consistency ({consistency_weight:.2f}) and Uniqueness ({uniqueness_weight:.2f}) by 50%")
            print(f"   üöÄ ANALYTICAL BOOST: Consistency √ó1.5, Uniqueness √ó1.5")
        
        # Assign weights
        weights = {
            'completeness': completeness_weight,
            'uniqueness': uniqueness_weight,
            'validity': validity_weight,
            'consistency': consistency_weight,
            'accuracy': accuracy_weight,
            'timeliness': timeliness_weight,
            'integrity': integrity_weight
        }
        
        # ==========================================
        # STEP 4: NORMALIZE TO SUM = 1.0
        # ==========================================
        total_weight = sum(weights.values())
        normalized_weights = {k: round(v / total_weight, 4) for k, v in weights.items()}
        
        reasoning_log.append(f"Normalized weights (sum=1.0): {normalized_weights}")
        
        print(f"\nüéØ Weight Normalization:")
        print(f"   Raw sum: {total_weight:.4f}")
        print(f"   Normalized sum: {sum(normalized_weights.values()):.4f}")
        print("\n‚úÖ Final Normalized Weights:")
        for dim, weight in normalized_weights.items():
            print(f"   - {dim.capitalize():15s}: {weight:.4f} ({weight*100:.2f}%)")
        print("üîç"*40 + "\n")
        
        # Store reasoning log for API response
        self.weight_reasoning_log = {
            'inferred_context': inferred_context,
            'context_scores': context_scores,
            'matched_keywords': transactional_matches if inferred_context == 'transactional' else master_matches if inferred_context == 'master' else analytical_matches,
            'reasoning_steps': reasoning_log,
            'column_schema_sample': self.df.columns.tolist()[:20]
        }
        
        return normalized_weights

    
    def calculate_all_dimensions(self, custom_weights: Optional[Dict[str, float]] = None) -> Dict[str, Any]:
        """Calculate all 6 data quality dimensions with real LLM insights."""
        
        # Step 1: Compute all dimensions (REAL COMPUTATION ONLY)
        try:
            dimensions = {
                'completeness': self.calculate_completeness(),
                'uniqueness': self.calculate_uniqueness(),
                'validity': self.calculate_validity(),
                'consistency': self.calculate_consistency(),
                'accuracy': self.calculate_accuracy(),
                'timeliness': self.calculate_timeliness(),
                'integrity': self.calculate_integrity()
            }
        except Exception as e:
            return {
                'status': 'insufficient_data',
                'error': str(e),
                'message': 'Unable to compute quality dimensions - real computation failed'
            }
        
        # Step 2: Extract dimension scores (NO HARDCODING)
        dimension_scores = {dim: dimensions[dim]['score'] for dim in dimensions}
        
        # Validate all scores are in 0-100 range
        for dim, score in dimension_scores.items():
            if not (0 <= score <= 100):
                raise ValueError(f"Score normalization failed for {dim}: {score} not in [0, 100]")
        
    def calculate_all_dimensions(self, custom_weights: Optional[Dict[str, float]] = None) -> Dict[str, Any]:
        """Calculate all 6 data quality dimensions with DYNAMIC weights based on EDA."""
        
        # Step 1: Calculate dynamic weights based on data characteristics
        if custom_weights is None:
            weights = self.calculate_dynamic_weights()
        else:
            weights = custom_weights
            print("‚ö†Ô∏è  Using custom weights provided by user")
        
        # Step 2: Compute all dimensions (REAL COMPUTATION ONLY)
        try:
            dimensions = {
                'completeness': self.calculate_completeness(),
                'uniqueness': self.calculate_uniqueness(),
                'validity': self.calculate_validity(),
                'consistency': self.calculate_consistency(),
                'accuracy': self.calculate_accuracy(),
                'timeliness': self.calculate_timeliness(),
                'integrity': self.calculate_integrity()
            }
        except Exception as e:
            return {
                'status': 'insufficient_data',
                'error': str(e),
                'message': 'Unable to compute quality dimensions - real computation failed'
            }
        
        # Step 3: Extract dimension scores (NO HARDCODING)
        dimension_scores = {dim: dimensions[dim]['score'] for dim in dimensions}
        
        # Validate all scores are in 0-100 range
        for dim, score in dimension_scores.items():
            if not (0 <= score <= 100):
                raise ValueError(f"Score normalization failed for {dim}: {score} not in [0, 100]")
        
        # Step 4: Calculate WEIGHTED overall score
        overall_score = sum(dimension_scores[dim] * weights[dim] for dim in dimension_scores)
        
        print("\n" + "="*80)
        print("üìä OVERALL QUALITY SCORE CALCULATION")
        print("="*80)
        print(f"‚öñÔ∏è  Weighting Strategy: DYNAMIC WEIGHTS (EDA-Based)")
        print(f"üìê Formula: Œ£(dimension_score √ó weight)")
        print(f"\nDimension Scores:")
        for dim, score in dimension_scores.items():
            weight = weights[dim]
            contribution = score * weight
            print(f"   - {dim.capitalize():15s}: {score:6.2f}/100 √ó {weight:.4f} = {contribution:6.2f} points")
        print(f"\nüéØ Overall Score: {overall_score:.2f}/100")
        print(f"   Calculation: " + " + ".join([f"({dimension_scores[d]:.2f} √ó {weights[d]:.4f})" for d in dimension_scores]))
        print("="*80 + "\n")
        
        # Step 5: Collect issues for LLM context
        issues_detected = self._extract_issues(dimensions)
        
        # Step 6: Generate REAL LLM insights (NO SIMULATION)
        try:
            llm_insights = self._generate_llm_insights(dimension_scores, issues_detected, weights)
        except Exception as e:
            llm_insights = {
                'status': 'external_dependency_failure',
                'error': str(e),
                'message': 'LLM API call failed - cannot generate insights'
            }
        
        # Step 6: Determine confidence level based on data completeness
        confidence_level = self._calculate_confidence_level(dimensions)
        
        # Step 7: Include validation errors and warnings in response
        validation_summary = {
            'errors': self.validation_errors,
            'warnings': self.validation_warnings,
            'critical_failures': self.critical_failures,
            'error_count': len(self.validation_errors),
            'warning_count': len(self.validation_warnings),
            'critical_count': len(self.critical_failures),
            'has_issues': len(self.validation_errors) > 0 or len(self.critical_failures) > 0
        }
        
        return {
            'scores': dimension_scores,
            'overall_score': round(overall_score, 2),
            'dimensions': dimensions,
            'issues_detected': issues_detected,
            'llm_insights': llm_insights,
            'confidence_level': confidence_level,
            'weights_applied': weights,  # Store actual dynamic weights used
            'reasoning_log': getattr(self, 'weight_reasoning_log', {}),  # Auditability requirement
            'quality_grade': self._get_quality_grade(overall_score),
            'validation_summary': validation_summary,  # NEW: Include all validation errors
            'summary': {
                'total_rows': self.total_rows,
                'total_columns': self.total_cols,
                'total_cells': self.total_cells
            },
            'computation_metadata': {
                'all_scores_normalized': True,
                'no_simulation': True,
                'llm_enabled': bool(self.api_key),
                'weighting_strategy': 'context_aware_dynamic',
                'timestamp': datetime.now().isoformat()
            }
        }
    
    def _extract_issues(self, dimensions: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Extract detected issues from dimension analysis."""
        issues = []
        
        # Completeness issues
        if dimensions['completeness']['score'] < 80:
            issues.append({
                'dimension': 'completeness',
                'severity': 'high' if dimensions['completeness']['score'] < 60 else 'medium',
                'description': f"{dimensions['completeness']['missing_cells']} missing cells detected",
                'affected_columns': dimensions['completeness']['total_columns_with_missing'] if 'total_columns_with_missing' in dimensions['completeness'] else 0
            })
        
        # Uniqueness issues
        if dimensions['uniqueness']['score'] < 95:
            issues.append({
                'dimension': 'uniqueness',
                'severity': 'high' if dimensions['uniqueness']['duplicate_rows'] > self.total_rows * 0.1 else 'medium',
                'description': f"{dimensions['uniqueness']['duplicate_rows']} duplicate records found",
                'duplicate_percentage': dimensions['uniqueness']['duplicate_percentage']
            })
        
        # Validity issues
        if dimensions['validity']['total_validations'] > 0 and dimensions['validity']['score'] < 90:
            issues.append({
                'dimension': 'validity',
                'severity': 'high' if dimensions['validity']['score'] < 70 else 'medium',
                'description': f"{dimensions['validity']['failed_validations']} validation failures",
                'validations_run': dimensions['validity']['total_validations']
            })
        
        # Accuracy issues (outliers)
        if dimensions['accuracy']['outliers_detected'] > 0:
            issues.append({
                'dimension': 'accuracy',
                'severity': 'medium',
                'description': f"{dimensions['accuracy']['outliers_detected']} outliers detected by Isolation Forest",
                'outlier_percentage': dimensions['accuracy']['outlier_percentage']
            })
        
        # Timeliness issues
        if dimensions['timeliness']['score'] < 60:
            issues.append({
                'dimension': 'timeliness',
                'severity': 'high' if dimensions['timeliness']['score'] < 40 else 'medium',
                'description': 'Data freshness is poor',
                'recommendation': dimensions['timeliness'].get('recommendation', 'Refresh data')
            })
        
        return issues
    
    def _calculate_confidence_level(self, dimensions: Dict[str, Any]) -> str:
        """Calculate confidence level based on data availability."""
        # High confidence: >90% complete, >95% unique, low outliers
        completeness = dimensions['completeness']['score']
        uniqueness = dimensions['uniqueness']['score']
        
        if completeness >= 90 and uniqueness >= 95:
            return 'high'
        elif completeness >= 70 and uniqueness >= 85:
            return 'medium'
        else:
            return 'low'
    
    def _generate_llm_insights(self, dimension_scores: Dict[str, float], issues: List[Dict], weights: Dict[str, float]) -> Dict[str, Any]:
        """Generate real insights using LLM API - NO SIMULATION."""
        
        if not self.api_key:
            return {
                'status': 'external_dependency_failure',
                'message': 'LLM API key not configured - cannot generate insights'
            }
        
        # Prepare context for LLM with dynamic weights and field-level data
        weights_info = "\n".join([f"- {dim.capitalize()}: {weight*100:.2f}% (priority: {'HIGH' if weight > 0.2 else 'MEDIUM' if weight > 0.15 else 'STANDARD'})" 
                                   for dim, weight in weights.items()])
        
        missing_columns = self._format_missing_columns()
        
        context = f"""You are a data quality remediation specialist for banking/financial datasets.

üìä DATASET METRICS:
- Total Rows: {self.total_rows:,}
- Total Columns: {self.total_cols}
- Column Names: {', '.join(self.df.columns.tolist()[:15])}{'...' if self.total_cols > 15 else ''}

üìâ DIMENSION SCORES (0-100):
- Completeness: {dimension_scores['completeness']:.2f}
- Uniqueness: {dimension_scores['uniqueness']:.2f}
- Validity: {dimension_scores['validity']:.2f}
- Consistency: {dimension_scores['consistency']:.2f}
- Accuracy: {dimension_scores['accuracy']:.2f}
- Timeliness: {dimension_scores['timeliness']:.2f}

‚öñÔ∏è DYNAMIC WEIGHTS (from EDA):
{weights_info}

‚ö†Ô∏è MISSING DATA BY COLUMN:
{missing_columns}

üî¥ DETECTED ISSUES:
{self._format_issues_for_llm(issues)}

üìã YOUR TASK:
Generate FIELD-SPECIFIC recommendations. For each problematic field:
1. Cite the EXACT field name from the dataset
2. Explain WHY this field is failing (root cause)
3. Explain the BUSINESS IMPACT (not technical jargon)
4. Provide a SPECIFIC remediation action

üì§ OUTPUT FORMAT (STRICT JSON):
{{
  "summary": "2-3 sentence executive summary focusing on highest-weight dimensions",
  "root_causes": [
    {{
      "field": "exact_field_name",
      "cause": "Technical reason for failure",
      "severity": "HIGH/MEDIUM/LOW"
    }}
  ],
  "recommended_actions": [
    {{
      "field": "exact_field_name",
      "why": "Root cause explanation",
      "impact": "Business consequence if not fixed",
      "action": "Specific remediation step",
      "priority": "HIGH/MEDIUM/LOW"
    }}
  ]
}}

‚ùå FORBIDDEN:
- Generic advice like "improve data quality" or "implement monitoring"
- Recommendations not tied to specific field names
- Technical jargon without business context

‚úÖ REQUIRED:
- Every recommendation MUST cite a specific field from the column list above
- Impact statements MUST explain business/regulatory consequences
- Actions MUST be implementable by a data engineering team
"""
        
        try:
            # Make REAL API call
            response = self._call_llm_api(context)
            
            # Parse LLM response
            insights = self._parse_llm_response(response)
            
            return insights
            
        except Exception as e:
            return {
                'status': 'external_dependency_failure',
                'error': str(e),
                'message': 'LLM API call failed'
            }
    
    def _format_missing_columns(self) -> str:
        """Format per-column missing data for LLM context."""
        missing_info = []
        for col in self.df.columns:
            missing_pct = self.df[col].isnull().mean() * 100
            if missing_pct >= 50:
                missing_info.append(f"- {col}: {missing_pct:.1f}% missing (CRITICAL)")
            elif missing_pct > 0:
                missing_info.append(f"- {col}: {missing_pct:.1f}% missing")
        
        if not missing_info:
            return "No significant missing data detected."
        
        return "\n".join(missing_info[:25])  # Limit to 25 columns
    
    def _format_issues_for_llm(self, issues: List[Dict]) -> str:
        """Format issues for LLM context."""
        if not issues:
            return "No critical issues detected."
        
        formatted = []
        for i, issue in enumerate(issues, 1):
            formatted.append(f"{i}. [{issue['severity'].upper()}] {issue['dimension']}: {issue['description']}")
        
        return "\n".join(formatted)
    
    def _call_llm_api(self, context: str, max_retries: int = 3) -> str:
        """Make LLM API call with exponential backoff retry."""
        import time
        
        headers = {
            'Authorization': f'Bearer {self.api_key}',
            'Content-Type': 'application/json'
        }
        
        payload = {
            'model': self.model,
            'messages': [
                {
                    'role': 'system',
                    'content': 'CRITICAL: You MUST respond ONLY in English. Do NOT use any Chinese characters, Chinese text, or any non-English characters whatsoever. All output must be in pure English ASCII characters only.\n\nYou are a data quality remediation specialist. Return ONLY valid JSON, no markdown code fences.'
                },
                {
                    'role': 'user',
                    'content': context
                }
            ],
            'temperature': 0.3,
            'max_tokens': 2000
        }
        
        last_error = None
        
        for attempt in range(max_retries):
            try:
                with httpx.Client(timeout=45.0) as client:
                    response = client.post(self.api_base, json=payload, headers=headers)
                    response.raise_for_status()
                    result = response.json()
                    return result['choices'][0]['message']['content']
                    
            except httpx.TimeoutException as e:
                last_error = e
                wait_time = 2 ** attempt
                print(f"‚è±Ô∏è LLM timeout on attempt {attempt + 1}/{max_retries}, retrying in {wait_time}s...")
                time.sleep(wait_time)
                
            except httpx.HTTPStatusError as e:
                last_error = e
                if e.response.status_code == 429:  # Rate limited
                    wait_time = 2 ** (attempt + 1)
                    print(f"‚ö†Ô∏è Rate limited, retrying in {wait_time}s...")
                    time.sleep(wait_time)
                elif e.response.status_code >= 500:  # Server error
                    wait_time = 2 ** attempt
                    print(f"‚ö†Ô∏è Server error {e.response.status_code}, retrying in {wait_time}s...")
                    time.sleep(wait_time)
                else:
                    raise  # Don't retry client errors (4xx except 429)
                    
            except Exception as e:
                last_error = e
                print(f"‚ö†Ô∏è LLM error on attempt {attempt + 1}: {str(e)}")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)
        
        # All retries exhausted - raise exception
        raise RuntimeError(f"LLM call failed after {max_retries} retries: {last_error}")
    
    def _parse_llm_response(self, response: str) -> Dict[str, Any]:
        """Parse LLM JSON response with text fallback."""
        import json as json_module
        
        # Try to extract JSON from response
        try:
            # Look for JSON block in markdown code fence
            json_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', response, re.DOTALL)
            if json_match:
                parsed = json_module.loads(json_match.group(1))
                return self._normalize_parsed_response(parsed)
            
            # Try to find raw JSON object
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                parsed = json_module.loads(json_match.group())
                return self._normalize_parsed_response(parsed)
        except json_module.JSONDecodeError as e:
            print(f"‚ö†Ô∏è JSON parse failed: {e}")
        
        # Fallback: text-based parsing
        print("‚ö†Ô∏è Using text fallback for LLM response parsing")
        return self._parse_llm_response_text(response)
    
    def _normalize_parsed_response(self, parsed: Dict) -> Dict[str, Any]:
        """Normalize parsed JSON to expected format."""
        return {
            'summary': parsed.get('summary', 'Analysis complete.'),
            'root_causes': parsed.get('root_causes', []),
            'recommended_actions': parsed.get('recommended_actions', [])
        }
    
    def _parse_llm_response_text(self, response: str) -> Dict[str, Any]:
        """Fallback text-based parsing for non-JSON responses."""
        lines = response.strip().split('\n')
        
        summary = []
        root_causes = []
        recommendations = []
        
        current_section = None
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            if 'summary' in line.lower() or 'executive' in line.lower():
                current_section = 'summary'
                continue
            elif 'root cause' in line.lower() or 'causes' in line.lower():
                current_section = 'root_causes'
                continue
            elif 'recommendation' in line.lower() or 'action' in line.lower():
                current_section = 'recommendations'
                continue
            
            if current_section == 'summary':
                if not line.startswith(('1.', '2.', '3.', '-', '*')):
                    summary.append(line)
            elif current_section == 'root_causes':
                if line.startswith(('1.', '2.', '3.', '-', '*')):
                    root_causes.append(line.lstrip('123.-* '))
            elif current_section == 'recommendations':
                if line.startswith(('1.', '2.', '3.', '-', '*')):
                    recommendations.append(line.lstrip('123.-* '))
        
        return {
            'summary': ' '.join(summary) if summary else response[:200],
            'root_causes': root_causes[:5] if root_causes else [],
            'recommended_actions': []  # No structured recommendations from text
        }
    
    def _get_quality_grade(self, score: float) -> str:
        """Convert score to letter grade."""
        if score >= 90:
            return 'A (Excellent)'
        elif score >= 80:
            return 'B (Good)'
        elif score >= 70:
            return 'C (Fair)'
        elif score >= 60:
            return 'D (Poor)'
        else:
            return 'F (Critical)'
